[project]
name = "LlaMa-Omni2-From-Scratch"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "accelerate>=1.7.0",
    "attention-sinks>=0.1.0",
    "blobfile>=3.0.0",
    "docker>=7.1.0",
    "duckduckgo-search>=8.0.2",
    "langchain>=0.3.25",
    "langchain-community>=0.3.21",
    "langchain-core>=0.3.62",
    "langchain-experimental>=0.3.4",
    "llama-index>=0.12.37",
    "llama-index-core>=0.12.37",
    "llama-index-embeddings-huggingface>=0.5.4",
    "llama-index-llms-ollama>=0.5.6",
    "llama-index-readers-file>=0.4.7",
    "mcp[cli]>=1.9.1",
    "nemo>=7.1.4",
    "numba>=0.61.2",
    "openai-whisper>=20240930",
    "optimum>=1.25.3",
    "pip>=25.1.1",
    "realtimestt>=0.3.104",
    "realtimetts[all]>=0.5.5",
    "scipy>=1.15.2",
    "sounddevice>=0.5.1",
    "soundfile>=0.13.1",
    "streaming-llm",
    "tiktoken>=0.9.0",
    "torch>=2.7.0",
    "torchaudio>=2.7.0",
    "tqdm>=4.67.1",
    "transformers>=4.51.3",
    "whisper>=1.1.10",
    "wikipedia>=1.4.0",
]

[tool.uv.sources]
streaming-llm = { path = "third_party/streaming-llm" }

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["mcp", "models", "utils", "tests", "config"]
